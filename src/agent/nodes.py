from src.agent.state import State
from src.agent.llm import llm_with_tools, llm_final_answer
from src.rag import rag

from langchain_core.messages import SystemMessage, HumanMessage

def judge_step(state: State) -> State:
    """
    LLM –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ª–∏ —Ç–µ–∫—É—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö RAG.
    –ù–ï–û–ë–•–û–î–ò–ú–û –≤—ã–∑–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç judge_rag_sufficiency.
    """

    # context_text = "\n\n".join(
    #     f"[source={c['source']}] {c['text']}"
    #     for c in state["rag_data"]
    # )

    context_text = "\n\n".join(
        f"{c}"
        for c in state["rag_data"]
    )

    system_prompt = """
–¢—ã ‚Äî –∞–≥–µ–Ω—Ç –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π RAG-—Å–∏—Å—Ç–µ–º—ã.

–¢–≤–æ—è –∑–∞–¥–∞—á–∞:
- –æ—Ü–µ–Ω–∏—Ç—å, –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, —á—Ç–æ–±—ã –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
- –µ—Å–ª–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ ‚Äî —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –û–î–ò–ù —Ç–æ—á–Ω—ã–π –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –ë–î

‚ö†Ô∏è **–¢—ã –û–ë–Ø–ó–ê–ù –≤—ã–∑–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é judge_rag_sufficiency**!
‚ö†Ô∏è **–î–µ–π—Å—Ç–≤—É–π —Å–æ–≥–ª–∞—Å–Ω–æ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏!**.
‚ö†Ô∏è **–ù–µ –æ—Ç–≤–µ—á–∞–π —Ç–µ–∫—Å—Ç–æ–º!**
"""
    print('----', 'judge_step')
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=f"""
–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:
{state['user_question']}

–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context_text}
""")
    ]

    # LLM –≤—ã–±–µ—Ä–µ—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –∏ –∑–∞–ø–æ–ª–Ω–∏—Ç –µ–≥–æ –∞—Ä–≥—É–º–µ–Ω—Ç—ã
    result = llm_with_tools.invoke(messages) 
    print("result.tool_calls:", result.tool_calls)

    if not result.tool_calls:
        print('fallback: LLM –Ω–µ —Å–º–æ–≥ –æ—Ü–µ–Ω–∏—Ç—å')
        state["sufficient"] = False
        state["followup_query"] = state["user_question"]
        state["confidence"] = 0.0
        return state

    tool_call = result.tool_calls[0]
    args = tool_call["args"]

    state["sufficient"] = args["sufficient"]
    state["followup_query"] = args.get("followup_query")
    state["confidence"] = args.get("confidence", 0.0)

    return state

def first_call_rag(state: State) -> State:
    """
    Retrieve new documents using followup_query
    """
    print('----', 'first_call_rag')

    query = state["user_question"]

    rag_answer = rag(user_query=query)

    state["rag_data"].append(rag_answer)
    state["iteration"] += 1
    print(state)

    return state

def rag_step(state: State) -> State:
    """
    Retrieve new documents using followup_query
    """
    print('----', 'rag_step')

    if state['iteration'] > 0:
        query = state["followup_query"]
    else:
        query = state["user_question"]

    # üîß RAG 
    rag_answer = rag(user_query=query)

    cur_chunk = {
        "text": rag_answer,
        "source": "rag_step",
        "query": query,
        "score": 1.0
    }

    state["rag_data"].append(cur_chunk)
    state["iteration"] += 1

    return state


def final_answer_step(state: State) -> State:
    """
    Final answering LLM
    """
    print('----', 'final_answer_step')

    context_text = "\n\n".join(
        f"{c}"
        for c in state["rag_data"]
    )

    prompt = [
        SystemMessage(content="""
–¢—ã ‚Äî –ø–æ–º–æ—â–Ω–∏–∫.
–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –∏—Å–ø–æ–ª—å–∑—É—è –¢–û–õ–¨–ö–û –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç.
–ù–µ –≤—ã–¥—É–º—ã–≤–∞–π —Ñ–∞–∫—Ç—ã.
"""),
        HumanMessage(content=f"""
–í–æ–ø—Ä–æ—Å:
{state['user_question']}

–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context_text}
""")
    ]

    answer = llm_final_answer.invoke(prompt)

    state["final_answer"] = answer.content
    return state