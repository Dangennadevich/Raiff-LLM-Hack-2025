

# from dotenv import load_dotenv
# from openai import OpenAI
from src.models import get_embedding, get_answer_with_retries

from tqdm import tqdm

from src.utils import data_to_storage, preprocess
from src.chunkinizer import chunkinizer
from src.faiss import build_faiss_hnsw_index, populate_faiss_index
from src.rag import rag_screach
from src.const import (
    TAGS_ANNOTATIONS_CHUNK_SIZE, 
    TAGS_ANNOTATIONS_OVERLAP, 
    TEXT_CHUNK_SIZE, 
    TEXT_OVERLAP, 
    ANNOTATION_EMBEDDINGS_DIMENSIONS, 
    TEXT_EMBEDDINGS_DIMENSIONS
    )

import pandas as pd
import numpy as np
import logging
import pickle
logging.basicConfig(
    level=logging.INFO,                 # –∏–ª–∏ DEBUG
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler()]  # –≤—ã–≤–æ–¥ –≤ –∫–æ–Ω—Å–æ–ª—å
)

logger = logging.getLogger(__name__)


# | = = | = = | = = | = = | = = | = = | = = | = = | = = | = = | = = | 2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö | = = | = = | = = | = = | = = | = = | = = | = = | = = | = = | = = |
logger.info("[Prepare data] Start...")

# –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
raw_data = pd.read_csv('train_data.csv')
# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
data = preprocess(raw_data)

logger.info("[Prepare data] End!")

# | = = | = = | = = | = = | = = | = = | = = | = = | = = | = = | = = | 3. –ß–∞–Ω–∫–∏ | = = | = = | = = | = = | = = | = = | = = | = = | = = | = = | = = |
logger.info("[Chunk] Start...")

# –°–æ–±–∏—Ä–∞–µ–º —á–∞–Ω–∫–∏ –¥–ª—è —Ç–µ–≥–æ–≤ –∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
chunks_ta = [
    chunkinizer(
            vanila_chunkinizer=1,
            text = f"{tags}. {annototions}", 
            chunk_size=TAGS_ANNOTATIONS_CHUNK_SIZE, 
            overlap_part=TAGS_ANNOTATIONS_OVERLAP,
            ) 
    for annototions, tags in zip(data['annotation'], data['tags'])
    ]

data['annotation_tags_chunk'] = chunks_ta

# –°–æ–±–∏—Ä–∞–µ–º —á–∞–Ω–∫–∏ –¥–ª—è —Ç–µ–∫—Å—Ç–∞
chunks_t = list()

for row in data['text']:
    doc_chunks = list()
    for doc in row:
        question, answer = doc[0], doc[1]
        doc_chunks.append(chunkinizer(
            vanila_chunkinizer=0,
            question=question, 
            answer=answer, 
            chunk_size=TEXT_CHUNK_SIZE, 
            overlap_part=TEXT_OVERLAP)
            )

    chunks_t.append(doc_chunks)

data['text_chunk'] = chunks_t

logger.info("[Chunk] End")

# | = = | = = | = = | = = | = = | = = | = = | = = | = = | = = | = = | 4. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ json –ë–î | = = | = = | = = | = = | = = | = = | = = | = = | = = | = = | = = |
logger.info("[JSON DB] Start...")

# –°–æ–∑–¥–∞–µ–º JSON  DB –≤ —Ñ–æ—Ä–º–∞—Ç–µ [–°–∫–≤–æ–∑–Ω–æ–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä : (doc_id, chunk)] –¥–ª—è —Ç–µ–≥–æ–≤ –∏ –Ω–Ω–æ—Ç–∞—Ü–∏–π
storage_an_t = data_to_storage(
        id_series = data['id'],
        data_series = data['annotation_tags_chunk']
    )

# –°–æ–∑–¥–∞–µ–º JSON  DB –≤ —Ñ–æ—Ä–º–∞—Ç–µ [–°–∫–≤–æ–∑–Ω–æ–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä : (doc_id, chunk)] –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —á–∞–Ω–∫–æ–≤
storage_t = data_to_storage(
        id_series = data['id'],
        data_series = data['text_chunk']
)

logger.info("[JSON DB] End")

# | = = | = = | = = | = = | = = | = = | = = | = = | = = | = = | = = | 5. –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–∏–Ω–≥–∏ | = = | = = | = = | = = | = = | = = | = = | = = | = = | = = | = = |
logger.info("[JSON DB] Get embeddings...")
logger.info("[JSON DB]     1. Annotations + tags")

### –î–ª—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
# –°–æ–±–µ—Ä–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è –±–∞—Ç—á–µ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
texts = list()
for _, val in storage_an_t.items():
    texts.append(val[1])

# –ó–∞–ø—Ä–æ—Å –∫ openrouter 
# embeddings = get_batch_embeddings(texts, dimensions=ANNOTATION_EMBEDDINGS_DIMENSIONS)
# # –°–æ–±–µ—Ä–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –≤–µ–∫—Ç–æ—Ä–æ–≤
# embed_storage_an_t = dict()
# for i in range(len(embeddings)):
#     embed_storage_an_t[i] = np.array(embeddings[i], np.float32)
with open('data/embed_storage_an_t_final.pickle', 'rb') as f:
    embed_storage_an_t = pickle.load(f)


logger.info("[JSON DB]     2. Text")
### –î–ª—è —Ç–µ–∫—Å—Ç–∞
# –°–æ–±–µ—Ä–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è –±–∞—Ç—á–µ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
texts = list()
for _, val in storage_t.items():
    texts.append(val[1])

# # –ó–∞–ø—Ä–æ—Å –∫ openrouter
    
### OLD
# embeddings = get_batch_embeddings(texts, dimensions=TEXT_EMBEDDINGS_DIMENSIONS)
### OLD

### New
batch_size = 32
embeddings_t = []

for i in range(0, len(texts), batch_size):
    batch = texts[i:i + batch_size]
    response = get_embedding(text=texts, batch_embeddings=True) # # –ó–∞–ø—Ä–æ—Å –∫ openrouter

    batch_embeddings = [item.embedding for item in response.data]
    embeddings_t.extend(batch_embeddings)
### New
    
# –°–æ–±–µ—Ä–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –≤–µ–∫—Ç–æ—Ä–æ–≤
embed_storage_t = dict()
for i in range(len(embeddings_t)):
    embed_storage_t[i] = np.array(embeddings_t[i], np.float32)

with open('data/embed_storage_t_final.pickle', 'rb') as f:
    embed_storage_t = pickle.load(f)

logger.info("[JSON DB] End")

# | = = | = = | = = | = = | = = | = = | = = | = = | = = | = = | = = | 6. Faiss | = = | = = | = = | = = | = = | = = | = = | = = | = = | = = | = = |

logger.info("[Faiss] Start...")

# Define the dimensions of the embedding vectors
embedding_dimension = ANNOTATION_EMBEDDINGS_DIMENSIONS  # Depends on the FastText model ANNOTATION_EMBEDDINGS_DIMENSIONS = 512
# Build the HNSW index
hnsw_index_an_t = build_faiss_hnsw_index(embedding_dimension)
# Populate the index from pd.Series
populate_faiss_index(index=hnsw_index_an_t, documents=embed_storage_an_t)

# Define the dimensions of the embedding vectors
embedding_dimension = TEXT_EMBEDDINGS_DIMENSIONS  # Depends on the FastText model
# Build the HNSW index
hnsw_index_t = build_faiss_hnsw_index(embedding_dimension)
# Populate the index from pd.Series
populate_faiss_index(index=hnsw_index_t, documents=embed_storage_t)

logger.info("[Faiss] End")

# | = = | = = | = = | = = | = = | = = | = = | = = | = = | = = | = = | 6. Call LLM | = = | = = | = = | = = | = = | = = | = = | = = | = = | = = | = = |

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--interactive", action="store_true", help="Run interactive chat mode")
    args = parser.parse_args()

    logger.info("[__main__] Start...")

    if args.interactive:
        # === –ò–ù–¢–ï–†–ê–ö–¢–ò–í–ù–´–ô –†–ï–ñ–ò–ú ===
        print("‚ö° –ó–∞–ø—É—â–µ–Ω –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º. –í–≤–µ–¥–∏—Ç–µ –≤–æ–ø—Ä–æ—Å.")
        print("–í–≤–µ–¥–∏—Ç–µ `exit` —á—Ç–æ–±—ã –≤—ã–π—Ç–∏.\n")

        while True:
            user_input = input("> ").strip()
            if user_input.lower() in ["exit", "quit"]:
                print("üëã –í—ã—Ö–æ–¥.")
                break

            # Router if easy question: rag_screach (classic), else decomposition

            prepared_question = rag_screach(
                user_query=user_input,
                hnsw_index_t=hnsw_index_t,
                hnsw_index_an_t=hnsw_index_an_t,
                storage_t=storage_t,
                storage_an_t=storage_an_t,
                data=data
            )

            answer = get_answer_with_retries(question=prepared_question)
            print(f"\n–û—Ç–≤–µ—Ç:\n{answer}\n")

    else:
        # === –ü–ê–ö–ï–¢–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê CSV ===
        questions = pd.read_csv('./questions.csv')
        questions_list = questions['–í–æ–ø—Ä–æ—Å'].tolist()
        answer_list = []

        for current_question in tqdm(questions_list, desc="–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤"):
            logger.info(f"[__main__] current_question: {current_question}")

            prepared_question = rag_screach(
                user_query=current_question,
                hnsw_index_t=hnsw_index_t,
                hnsw_index_an_t=hnsw_index_an_t,
                storage_t=storage_t,
                storage_an_t=storage_an_t,
                data=data
            )

            logger.info(f"[__main__] prepared_question: {prepared_question}")

            answer = get_answer_with_retries(question=prepared_question)
            logger.info(f"[__main__] answer: {answer}")

            answer_list.append(answer)

        questions['–û—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å'] = answer_list
        questions.to_csv('submission_v3_np.csv', index=False)
        print("‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ submission_v3_np.csv")